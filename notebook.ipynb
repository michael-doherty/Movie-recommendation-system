{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "credits = pd.read_csv('data/tmdb_5000_credits.csv')\n",
    "movies = pd.read_csv('data/tmdb_5000_movies.csv')\n",
    "movies = movies.merge(credits, on='title')\n",
    "movies = movies.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "def col_clean(c, subkey='name'):\n",
    "    ls =[]\n",
    "    c = json.loads(c)\n",
    "    for genre in c:\n",
    "        ls.append(genre[subkey])\n",
    "    return ls\n",
    "\n",
    "\n",
    "movies['genres'] = movies['genres'].apply(col_clean)\n",
    "movies['keywords'] = movies['keywords'].apply(col_clean)\n",
    "movies['production_companies'] = movies['production_companies'].apply(col_clean)\n",
    "movies['production_countries'] = movies['production_countries'].apply(col_clean)\n",
    "movies['spoken_languages'] = movies['spoken_languages'].apply(col_clean)\n",
    "movies['characters'] = movies['cast'].apply(col_clean, subkey='character')\n",
    "movies['cast'] = movies['cast'].apply(col_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['keywords_strings'] = [' '.join(i) for i in movies['keywords']]\n",
    "movies['genres_strings'] = [' '.join(i) for i in movies['genres']]\n",
    "movies['characters_strings'] = [' '.join(i[:10]) for i in movies['characters']]\n",
    "movies['cast_strings'] = [' '.join(i[:10]) for i in movies['cast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['content'] = movies['keywords_strings'] + ' ' + movies['overview'] + ' ' + movies['genres_strings'] + ' ' + movies['characters_strings'] + ' ' + movies['cast_strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = movies['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       culture clash future space war space colony so...\n",
       "1       ocean drug abuse exotic island east india trad...\n",
       "2       spy based on novel secret agent sequel mi6 bri...\n",
       "3       dc comics crime fighter terrorist secret ident...\n",
       "4       based on novel mars medallion space travel pri...\n",
       "                              ...                        \n",
       "4779    salesclerk loser aftercreditsstinger Convenien...\n",
       "4787    dating divorce sex scene sex comedy anti roman...\n",
       "4797    home invasion Recently dumped by his girlfiren...\n",
       "4802    distrust garage identity crisis time travel ti...\n",
       "4807     When ambitious New York attorney Sam is sent ...\n",
       "Name: content, Length: 1494, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract features using BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "batch_size = 16  # reduce batch size to avoid OOM error\n",
    "max_length = 128 # set maximum sequence length\n",
    "encoded_content = []\n",
    "for i in range(0, len(content), batch_size):\n",
    "    batch = content[i:i+batch_size]\n",
    "    encoded_batch = tokenizer.batch_encode_plus(batch, add_special_tokens=True, truncation=True, max_length=max_length, padding='longest', return_tensors='tf')\n",
    "    encoded_content.append(encoded_batch)\n",
    "\n",
    "embeddings = []\n",
    "for batch in encoded_content:\n",
    "    input_ids = batch['input_ids']\n",
    "    token_type_ids = batch['token_type_ids']\n",
    "    embeddings_batch = model(input_ids, token_type_ids=token_type_ids)[0][:, 0, :]\n",
    "    embeddings.append(embeddings_batch)\n",
    "\n",
    "embeddings = tf.concat(embeddings, axis=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use PCA for feature reduction\n",
    "pca = PCA(n_components=100)\n",
    "pca_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate similarity scores\n",
    "similarity_matrix = cosine_similarity(pca_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content_id:14\n",
      "Recommendations similar to The Avengers:\n",
      "\n",
      "- Avengers: Age of Ultron\n",
      "- X-Men: Days of Future Past\n",
      "- Ant-Man\n",
      "- X-Men: Apocalypse\n",
      "- Suicide Squad\n",
      "- Man of Steel\n",
      "- Captain America: The First Avenger\n",
      "- Batman v Superman: Dawn of Justice\n",
      "- Captain America: The Winter Soldier\n",
      "- Fantastic Four\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Build the recommender system 299, 78, 69, 14\n",
    "from random import randint\n",
    "content_id = randint(0,len(movies))\n",
    "num_recommendations = 10\n",
    "similar_content_ids = np.argsort(-similarity_matrix[content_id])[:num_recommendations+1]\n",
    "similar_content_ids = similar_content_ids[similar_content_ids != content_id]\n",
    "\n",
    "base_movie = movies.iloc[content_id]['title']\n",
    "print(f'Content_id:{content_id}')\n",
    "print(f'Recommendations similar to {base_movie}:')\n",
    "print('')\n",
    "for i in similar_content_ids:\n",
    "    print('- ' + movies.iloc[i]['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
